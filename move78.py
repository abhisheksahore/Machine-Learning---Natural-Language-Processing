# -*- coding: utf-8 -*-
"""move78.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gci71WAySF-Of65jQNiQxTCUbzFskTpc
"""

from google.colab import drive

drive.mount('/content/drive/')

"""## Importing libraries"""

import nltk
import pandas
import matplotlib as plt
import numpy as np

"""## Loading dataset"""

dataframe = pandas.read_csv("/content/drive/My Drive/Colab Notebooks/data-training.csv")

dataframe.sample(25)
print(dataframe.iloc[73,0])

from collections import Counter
Counter(dataframe['Label'])

X = dataframe.iloc[:,0].values
Y = dataframe.iloc[:,1].values

"""## Pre-Processing starts here"""

#pre-processing


import re 
def cleaning(t):
    t = re.sub(r"\n", "", t)    
    t = re.sub(r"\r", "", t)  
    t = re.sub(r"[|{}()#!*+=<>]", "", t)
    return t.strip()
X_ = []
for i in range(0, len(X)):
    X_.append(cleaning(X[i]))

"""## Spliting data into train and test data"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_, Y, test_size = .20, random_state = 42)

X_train[18]

X_train = pandas.Series(X_train)
X_test = pandas.Series(X_test)

X_train.sample(25)

print(X_train[82])
print(Y_train[82])

X_train = X_train.str.lower()
X_train.sample(10)

X_test = X_test.str.lower()
X_test.sample(10)

X_ = [i.lower() for i in X_]

"""## Importing some more libraries"""

from sklearn.pipeline import Pipeline
from sklearn.multiclass import OneVsRestClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
from sklearn.model_selection import GridSearchCV
from nltk.tokenize import TreebankWordTokenizer

"""## Creating Pipelines and vectorization"""

tokenizer = TreebankWordTokenizer()

# Pipeline for Random Forest Classifier
model1 = Pipeline([('count_vect', CountVectorizer(tokenizer = tokenizer.tokenize)),
                 ('tfidf_trans', TfidfTransformer()),
                 ('clf1', RandomForestClassifier())])

# Pipeline for K Neighbors Classifier 
model2 = Pipeline([('count_vect', CountVectorizer(tokenizer = tokenizer.tokenize)),
                 ('tfidf_trans', TfidfTransformer()),
                 ('clf2', KNeighborsClassifier())])

#Pipeline for MultinomialNB Classifier
model3 = Pipeline([('count_vect', CountVectorizer(tokenizer = tokenizer.tokenize)),
                 ('tfidf_trans', TfidfTransformer()),
                 ('clf3', MultinomialNB())])

"""## Giving hyperparameters for Hyperparameter tuning"""

parameter1 = {# for countvectorizer
              'count_vect__stop_words': ['english'],
              'count_vect__ngram_range': [(1,1),(1,2),(2,2)],
              'count_vect__max_df': [i for i in np.arange(0.4,0.6,0.05)],
              'count_vect__min_df': [i for i in np.arange(2,25,5)],
              # for tfidfTransformer
              "tfidf_trans__norm": ['l1', 'l2', None],
              'tfidf_trans__use_idf': [True, False],
              # for clf1
              'clf1__n_estimators': [100],
              'clf1__criterion': ['gini', 'entropy'],
              'clf1__max_depth': [None, 3, 5],
              'clf1__n_jobs': [None]
             }

parameter2 = {# for countvectorizer
              "count_vect__stop_words": ['english'],
              "count_vect__ngram_range": [(1,1),(1,2),(2,2)],
              "count_vect__max_df": [i for i in np.arange(0.4,0.6,0.05)],
              "count_vect__min_df": [i for i in np.arange(2,25,5)],
              # for tfidfTransformer
              "tfidf_trans__norm": ['l1', 'l2', None],
              "tfidf_trans__use_idf": [True, False],
              # for clf2
              "clf2__n_neighbors": [i for i in np.arange(3,10)]
              }

parameter3 = {# for countvectorizer
              "count_vect__stop_words": ['english'],
              "count_vect__ngram_range": [(1,1),(1,2),(2,2)],
              "count_vect__max_df": [i for i in np.arange(0.4,0.6,0.05)],
              "count_vect__min_df": [i for i in np.arange(2,25,5)],
              # for tfidfTransformer
              "tfidf_trans__norm": ['l1', 'l2', None],
              "tfidf_trans__use_idf": [True, False],
              # for clf3
              "clf3__alpha": [0.0,0.2,0.4,0.6,0.8,1.0],
              "clf3__fit_prior": [True, False]
             }

"""## Checking the best hyperparameters for Random_Forest_Classifier"""

grid1 = GridSearchCV(estimator = model1, param_grid = parameter1,  scoring = 'accuracy', n_jobs = -1, cv = 5)
grid1 = grid1.fit(X_, Y)
print("_Random_Forest_Classifier_")
print(grid1.best_score_)
print(grid1.best_params_)

"""## Checking the best hyperparameters for K_Neighbors_Classifier"""

grid2 = GridSearchCV(estimator = model2, param_grid = parameter2,  scoring = 'accuracy', n_jobs = -1, cv = 5)
grid2 = grid2.fit(X_, Y)
print("_K_Neighbors_Classifier_")
print(grid2.best_score_)
print(grid2.best_params_)

"""## Checking the best hyperparameters for MultinomialNB"""

grid3 = GridSearchCV(estimator = model3, param_grid = parameter3,  scoring = 'accuracy', n_jobs = -1, cv = 5)
grid3 = grid3.fit(X_, Y)
print("_MultinomialNB_")
print(grid3.best_score_)
print(grid3.best_params_)

"""#Preparing the final pipeline using the selected hyperparameters"""

model = Pipeline([('count_vect', CountVectorizer(tokenizer = tokenizer.tokenize, stop_words='english', min_df=2, max_df=0.55, ngram_range=(1,1))),
                 ('tfidf_trans', TfidfTransformer(norm='l1', use_idf=True)),
                 ('clf', RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None, n_jobs=None))])

model.fit(X_train, Y_train)

pred = model.predict(X_test)

pred

"""#Creating Confusion matrix and checking the accuracy of the model"""

from sklearn.metrics import confusion_matrix, accuracy_score
confusion_matrix(Y_test, pred)

accuracy = accuracy_score(Y_test, pred)

print(accuracy)

text = input()
model.predict([text])

